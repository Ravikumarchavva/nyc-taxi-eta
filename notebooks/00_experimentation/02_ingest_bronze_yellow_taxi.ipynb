{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c074d08e",
   "metadata": {},
   "source": [
    "# NYC Yellow Taxi - Bronze Layer Ingestion Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **Bronze Layer** of the Medallion Architecture for NYC Yellow Taxi trip data.\n",
    "\n",
    "### Pipeline Characteristics\n",
    "\n",
    "| Attribute        | Value                                          |\n",
    "| ---------------- | ---------------------------------------------- |\n",
    "| **Source**       | NYC TLC Yellow Taxi Parquet files (2009-2025)  |\n",
    "| **Target**       | Delta Lake Bronze Table                        |\n",
    "| **Processing**   | Batch ingestion with schema evolution handling |\n",
    "| **Partitioning** | By Year                                        |\n",
    "\n",
    "### Schema Evolution Timeline\n",
    "\n",
    "- **2009-2014**: Old format with lat/lon coordinates (`Start_Lon`, `End_Lat`, etc.)\n",
    "- **2015+**: New format with LocationIDs (`PULocationID`, `DOLocationID`)\n",
    "- **2015+**: Added `improvement_surcharge`\n",
    "- **2019+**: Added `congestion_surcharge`\n",
    "- **2025+**: Added `Airport_fee`, `cbd_congestion_fee`\n",
    "\n",
    "### Bronze Layer Principles\n",
    "\n",
    "1. **Raw data preservation** - All columns cast to STRING\n",
    "2. **Schema-on-read** - No type enforcement at ingestion\n",
    "3. **Full lineage** - Source file tracking via `source_file` column\n",
    "4. **Idempotent** - Re-runnable without side effects\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939f0a5",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:20 | INFO     | ============================================================\n",
      "2025-11-29 14:40:20 | INFO     | Pipeline: nyc_yellow_taxi_bronze_ingestion v1.2.0\n",
      "2025-11-29 14:40:20 | INFO     | Environment: development\n",
      "2025-11-29 14:40:20 | INFO     | Started at: 2025-11-29T14:40:20.832415\n",
      "2025-11-29 14:40:20 | INFO     | Memory: Driver=64g, Executor=64g\n",
      "2025-11-29 14:40:20 | INFO     | Batch size: 20 files per batch\n",
      "2025-11-29 14:40:20 | INFO     | Schema versions: V1 (2009), V2 (2010), V3 (2011+)\n",
      "2025-11-29 14:40:20 | INFO     | ============================================================\n",
      "2025-11-29 14:40:20 | INFO     | Pipeline: nyc_yellow_taxi_bronze_ingestion v1.2.0\n",
      "2025-11-29 14:40:20 | INFO     | Environment: development\n",
      "2025-11-29 14:40:20 | INFO     | Started at: 2025-11-29T14:40:20.832415\n",
      "2025-11-29 14:40:20 | INFO     | Memory: Driver=64g, Executor=64g\n",
      "2025-11-29 14:40:20 | INFO     | Batch size: 20 files per batch\n",
      "2025-11-29 14:40:20 | INFO     | Schema versions: V1 (2009), V2 (2010), V3 (2011+)\n",
      "2025-11-29 14:40:20 | INFO     | ============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and environment setup for Bronze layer ingestion.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PIPELINE CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Centralized configuration for the Bronze ingestion pipeline.\"\"\"\n",
    "\n",
    "    # Pipeline metadata\n",
    "    pipeline_name: str = \"nyc_yellow_taxi_bronze_ingestion\"\n",
    "    pipeline_version: str = \"1.3.0\"  # Updated for AMD EPYC optimization\n",
    "    environment: str = field(default_factory=lambda: os.getenv(\"ENV\", \"development\"))\n",
    "\n",
    "    # Spark configuration - AMD EPYC 7742 (64 cores) + 128GB RAM\n",
    "    # Reserve 8 cores for Linux GUI + OS, use 56 for Spark\n",
    "    app_name: str = \"NYCTaxiBronze\"\n",
    "    driver_memory: str = \"16g\"    # Driver just coordinates\n",
    "    executor_memory: str = \"80g\"  # Executor does heavy lifting\n",
    "    executor_cores: int = 56      # 64 - 8 reserved for OS/GUI\n",
    "    parallelism: int = 56         # Match available cores\n",
    "    shuffle_partitions: int = 112 # 2x cores for shuffle operations\n",
    "    \n",
    "    # Batch processing - process files in chunks to avoid OOM\n",
    "    batch_size: int = 20  # Process 20 files at a time\n",
    "\n",
    "    # Data paths (will be set from settings)\n",
    "    source_dir: Optional[Path] = None\n",
    "    target_dir: Optional[Path] = None\n",
    "\n",
    "    # Processing options\n",
    "    write_mode: str = \"overwrite\"  # overwrite, append\n",
    "    partition_by: List[str] = field(default_factory=lambda: [\"year\"])\n",
    "    enable_data_quality_checks: bool = True\n",
    "\n",
    "    # Schema version detection by year\n",
    "    # V1: 2009 (vendor_name, Trip_Pickup_DateTime)\n",
    "    # V2: 2010 (vendor_id, pickup_datetime) \n",
    "    # V3: 2011+ (VendorID, tpep_pickup_datetime)\n",
    "    v1_years: List[str] = field(default_factory=lambda: [\"2009\"])\n",
    "    v2_years: List[str] = field(default_factory=lambda: [\"2010\"])\n",
    "    # All other years are V3 (new format)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        assert self.write_mode in [\"overwrite\", \"append\"], (\n",
    "            f\"Invalid write_mode: {self.write_mode}\"\n",
    "        )\n",
    "        assert self.driver_memory.endswith(\"g\"), (\n",
    "            \"Memory should be specified in gigabytes (e.g., '8g')\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "def setup_logging(config: PipelineConfig) -> logging.Logger:\n",
    "    \"\"\"Configure logging for the pipeline.\"\"\"\n",
    "    logger = logging.getLogger(config.pipeline_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "\n",
    "    # Console handler with formatting\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\n",
    "        fmt=\"%(asctime)s | %(levelname)-8s | %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Initialize configuration and logger\n",
    "config = PipelineConfig()\n",
    "logger = setup_logging(config)\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Pipeline: {config.pipeline_name} v{config.pipeline_version}\")\n",
    "logger.info(f\"Environment: {config.environment}\")\n",
    "logger.info(f\"Started at: {datetime.now().isoformat()}\")\n",
    "logger.info(f\"Memory: Driver={config.driver_memory}, Executor={config.executor_memory}\")\n",
    "logger.info(f\"CPU: {config.executor_cores}/64 cores (8 reserved for OS/GUI)\")\n",
    "logger.info(f\"Batch size: {config.batch_size} files per batch\")\n",
    "logger.info(\"Schema versions: V1 (2009), V2 (2010), V3 (2011+)\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb95c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:21 | INFO     | Initializing Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/administrator/Desktop/datascience/github/nyc-taxi-eta/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/administrator/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/administrator/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f54775f7-b2c0-46fb-b231-9c7f10d4ba46;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      ":: loading settings :: url = jar:file:/home/administrator/Desktop/datascience/github/nyc-taxi-eta/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/administrator/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/administrator/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f54775f7-b2c0-46fb-b231-9c7f10d4ba46;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 163ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f54775f7-b2c0-46fb-b231-9c7f10d4ba46\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 163ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f54775f7-b2c0-46fb-b231-9c7f10d4ba46\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/11/29 14:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 14:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:26 | INFO     | Spark version: 4.0.0\n",
      "2025-11-29 14:40:26 | INFO     | Configured driver memory: 64g\n",
      "2025-11-29 14:40:26 | INFO     | Configured executor memory: 64g\n",
      "2025-11-29 14:40:26 | INFO     | Off-heap memory: 16g\n",
      "2025-11-29 14:40:26 | INFO     | Configured driver memory: 64g\n",
      "2025-11-29 14:40:26 | INFO     | Configured executor memory: 64g\n",
      "2025-11-29 14:40:26 | INFO     | Off-heap memory: 16g\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Session initialization with Delta Lake support.\n",
    "\n",
    "⚠️ IMPORTANT: You MUST restart the kernel for memory/CPU changes to take effect!\n",
    "   Kernel → Restart Kernel, then run all cells from the beginning.\n",
    "\n",
    "Optimized for: AMD EPYC 7742 (64 cores) + 128GB RAM\n",
    "\"\"\"\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "import pyspark.sql\n",
    "\n",
    "\n",
    "def create_spark_session(config: PipelineConfig) -> pyspark.sql.SparkSession:\n",
    "    \"\"\"\n",
    "    Create and configure Spark session with Delta Lake support.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration object\n",
    "\n",
    "    Returns:\n",
    "        Configured SparkSession\n",
    "    \"\"\"\n",
    "    logger.info(\"Initializing Spark session...\")\n",
    "\n",
    "    builder = (\n",
    "        pyspark.sql.SparkSession.builder.appName(config.app_name)\n",
    "        # Delta Lake extensions\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        # Memory configuration\n",
    "        .config(\"spark.driver.memory\", config.driver_memory)\n",
    "        .config(\"spark.executor.memory\", config.executor_memory)\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .config(\"spark.memory.fraction\", \"0.8\")\n",
    "        .config(\"spark.memory.storageFraction\", \"0.2\")\n",
    "        # Off-heap memory for extra headroom\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "        .config(\"spark.memory.offHeap.size\", \"16g\")\n",
    "        # CPU/Core configuration - AMD EPYC 7742 optimization\n",
    "        .config(\"spark.executor.cores\", str(config.executor_cores))\n",
    "        .config(\"spark.default.parallelism\", str(config.parallelism))\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(config.shuffle_partitions))\n",
    "        # Performance tuning\n",
    "        .config(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        # Delta optimizations\n",
    "        .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "        .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "        # File handling\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "    )\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"Driver memory: {config.driver_memory}\")\n",
    "    logger.info(f\"Executor memory: {config.executor_memory}\")\n",
    "    logger.info(f\"Executor cores: {config.executor_cores}\")\n",
    "    logger.info(f\"Default parallelism: {config.parallelism}\")\n",
    "    logger.info(f\"Shuffle partitions: {config.shuffle_partitions}\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cb702",
   "metadata": {},
   "source": [
    "## 2. Schema Registry\n",
    "\n",
    "Define versioned schemas for handling schema evolution across different data vintages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22986eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:26 | INFO     | Schema Registry initialized with 25 target columns\n",
      "2025-11-29 14:40:26 | INFO     | Supports 3 schema versions: V1 (2009), V2 (2010), V3 (2011+)\n",
      "2025-11-29 14:40:26 | INFO     | Supports 3 schema versions: V1 (2009), V2 (2010), V3 (2011+)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schema Registry for NYC Yellow Taxi data.\n",
    "\n",
    "Handles schema evolution from 2009 to present day.\n",
    "\n",
    "IMPORTANT: There are THREE distinct schema versions:\n",
    "- 2009: Old V1 format (vendor_name, Trip_Pickup_DateTime, Start_Lon, etc.)\n",
    "- 2010: Old V2 format (vendor_id, pickup_datetime, pickup_longitude, etc.)  \n",
    "- 2011+: New format (VendorID, tpep_pickup_datetime, PULocationID, etc.)\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "from pyspark.sql import Column, DataFrame\n",
    "from pyspark.sql.functions import col, lit, input_file_name\n",
    "\n",
    "\n",
    "class SchemaRegistry:\n",
    "    \"\"\"\n",
    "    Registry for managing schema versions and mappings.\n",
    "\n",
    "    This class handles the schema evolution of NYC TLC Yellow Taxi data\n",
    "    which changed multiple times between 2009 and 2011.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unified Bronze layer column order\n",
    "    BRONZE_COLUMNS: List[str] = [\n",
    "        \"vendor_id\",\n",
    "        \"pickup_datetime\",\n",
    "        \"dropoff_datetime\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"pickup_longitude\",\n",
    "        \"pickup_latitude\",\n",
    "        \"dropoff_longitude\",\n",
    "        \"dropoff_latitude\",\n",
    "        \"rate_code\",\n",
    "        \"store_and_fwd_flag\",\n",
    "        \"payment_type\",\n",
    "        \"fare_amount\",\n",
    "        \"extra\",\n",
    "        \"mta_tax\",\n",
    "        \"tip_amount\",\n",
    "        \"tolls_amount\",\n",
    "        \"total_amount\",\n",
    "        \"pulocationid\",\n",
    "        \"dolocationid\",\n",
    "        \"improvement_surcharge\",\n",
    "        \"congestion_surcharge\",\n",
    "        \"airport_fee\",\n",
    "        \"cbd_congestion_fee\",\n",
    "        \"source_file\",\n",
    "    ]\n",
    "\n",
    "    # =========================================================================\n",
    "    # SCHEMA V1: 2009 (vendor_name, Trip_Pickup_DateTime, Start_Lon)\n",
    "    # =========================================================================\n",
    "    V1_2009_MAPPING: Dict[str, str] = {\n",
    "        \"vendor_name\": \"vendor_id\",\n",
    "        \"Trip_Pickup_DateTime\": \"pickup_datetime\",\n",
    "        \"Trip_Dropoff_DateTime\": \"dropoff_datetime\",\n",
    "        \"Passenger_Count\": \"passenger_count\",\n",
    "        \"Trip_Distance\": \"trip_distance\",\n",
    "        \"Start_Lon\": \"pickup_longitude\",\n",
    "        \"Start_Lat\": \"pickup_latitude\",\n",
    "        \"End_Lon\": \"dropoff_longitude\",\n",
    "        \"End_Lat\": \"dropoff_latitude\",\n",
    "        \"Rate_Code\": \"rate_code\",\n",
    "        \"store_and_forward\": \"store_and_fwd_flag\",\n",
    "        \"Payment_Type\": \"payment_type\",\n",
    "        \"Fare_Amt\": \"fare_amount\",\n",
    "        \"surcharge\": \"extra\",\n",
    "        \"mta_tax\": \"mta_tax\",\n",
    "        \"Tip_Amt\": \"tip_amount\",\n",
    "        \"Tolls_Amt\": \"tolls_amount\",\n",
    "        \"Total_Amt\": \"total_amount\",\n",
    "    }\n",
    "\n",
    "    # =========================================================================\n",
    "    # SCHEMA V2: 2010 (vendor_id, pickup_datetime, pickup_longitude)\n",
    "    # =========================================================================\n",
    "    V2_2010_MAPPING: Dict[str, str] = {\n",
    "        \"vendor_id\": \"vendor_id\",\n",
    "        \"pickup_datetime\": \"pickup_datetime\",\n",
    "        \"dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"passenger_count\": \"passenger_count\",\n",
    "        \"trip_distance\": \"trip_distance\",\n",
    "        \"pickup_longitude\": \"pickup_longitude\",\n",
    "        \"pickup_latitude\": \"pickup_latitude\",\n",
    "        \"dropoff_longitude\": \"dropoff_longitude\",\n",
    "        \"dropoff_latitude\": \"dropoff_latitude\",\n",
    "        \"rate_code\": \"rate_code\",\n",
    "        \"store_and_fwd_flag\": \"store_and_fwd_flag\",\n",
    "        \"payment_type\": \"payment_type\",\n",
    "        \"fare_amount\": \"fare_amount\",\n",
    "        \"surcharge\": \"extra\",\n",
    "        \"mta_tax\": \"mta_tax\",\n",
    "        \"tip_amount\": \"tip_amount\",\n",
    "        \"tolls_amount\": \"tolls_amount\",\n",
    "        \"total_amount\": \"total_amount\",\n",
    "    }\n",
    "\n",
    "    # =========================================================================\n",
    "    # SCHEMA V3: 2011+ (VendorID, tpep_pickup_datetime, PULocationID)\n",
    "    # =========================================================================\n",
    "    V3_NEW_MAPPING: Dict[str, str] = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"passenger_count\": \"passenger_count\",\n",
    "        \"trip_distance\": \"trip_distance\",\n",
    "        \"RatecodeID\": \"rate_code\",\n",
    "        \"store_and_fwd_flag\": \"store_and_fwd_flag\",\n",
    "        \"payment_type\": \"payment_type\",\n",
    "        \"fare_amount\": \"fare_amount\",\n",
    "        \"extra\": \"extra\",\n",
    "        \"mta_tax\": \"mta_tax\",\n",
    "        \"tip_amount\": \"tip_amount\",\n",
    "        \"tolls_amount\": \"tolls_amount\",\n",
    "        \"total_amount\": \"total_amount\",\n",
    "        \"PULocationID\": \"pulocationid\",\n",
    "        \"DOLocationID\": \"dolocationid\",\n",
    "        \"improvement_surcharge\": \"improvement_surcharge\",\n",
    "        \"congestion_surcharge\": \"congestion_surcharge\",\n",
    "        \"Airport_fee\": \"airport_fee\",\n",
    "        \"cbd_congestion_fee\": \"cbd_congestion_fee\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def _build_select_exprs(cls, mapping: Dict[str, str], available_cols: List[str]) -> List[Column]:\n",
    "        \"\"\"Build SELECT expressions using a mapping and available columns.\"\"\"\n",
    "        exprs = []\n",
    "        for target_col in cls.BRONZE_COLUMNS:\n",
    "            if target_col == \"source_file\":\n",
    "                exprs.append(input_file_name().alias(target_col))\n",
    "            else:\n",
    "                # Find source column in mapping\n",
    "                source_col = next(\n",
    "                    (src for src, tgt in mapping.items() if tgt == target_col),\n",
    "                    None,\n",
    "                )\n",
    "                if source_col and source_col in available_cols:\n",
    "                    # Cast to string to handle type variations\n",
    "                    exprs.append(col(source_col).cast(\"string\").alias(target_col))\n",
    "                else:\n",
    "                    exprs.append(lit(None).cast(\"string\").alias(target_col))\n",
    "        return exprs\n",
    "\n",
    "    @classmethod\n",
    "    def transform_v1_2009(cls, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Transform 2009 V1 format data to Bronze schema.\"\"\"\n",
    "        exprs = cls._build_select_exprs(cls.V1_2009_MAPPING, df.columns)\n",
    "        return df.select(*exprs)\n",
    "\n",
    "    @classmethod\n",
    "    def transform_v2_2010(cls, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Transform 2010 V2 format data to Bronze schema.\"\"\"\n",
    "        exprs = cls._build_select_exprs(cls.V2_2010_MAPPING, df.columns)\n",
    "        return df.select(*exprs)\n",
    "\n",
    "    @classmethod\n",
    "    def transform_v3_new(cls, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Transform 2011+ new format data to Bronze schema.\"\"\"\n",
    "        exprs = cls._build_select_exprs(cls.V3_NEW_MAPPING, df.columns)\n",
    "        return df.select(*exprs)\n",
    "\n",
    "\n",
    "# Initialize schema registry\n",
    "schema_registry = SchemaRegistry()\n",
    "logger.info(f\"Schema Registry initialized with {len(SchemaRegistry.BRONZE_COLUMNS)} target columns\")\n",
    "logger.info(\"Supports 3 schema versions: V1 (2009), V2 (2010), V3 (2011+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbda4f8",
   "metadata": {},
   "source": [
    "## 3. Data Quality Framework\n",
    "\n",
    "Implement data quality checks to validate data before and after transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f9ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:26 | INFO     | Data Quality Framework initialized\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Quality Framework for Bronze layer validation.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count, col, sum as spark_sum, when, isnan, isnull\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataQualityResult:\n",
    "    \"\"\"Results from data quality checks.\"\"\"\n",
    "\n",
    "    check_name: str\n",
    "    passed: bool\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        status = \"✅ PASSED\" if self.passed else \"❌ FAILED\"\n",
    "        return f\"{status} | {self.check_name}: {self.details}\"\n",
    "\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Enterprise-grade data quality checker for Spark DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "        self.results: List[DataQualityResult] = []\n",
    "\n",
    "    def check_not_empty(self, df: DataFrame, name: str) -> DataQualityResult:\n",
    "        \"\"\"Check that DataFrame is not empty.\"\"\"\n",
    "        # Use limit(1) to avoid full scan\n",
    "        is_empty = df.limit(1).count() == 0\n",
    "        result = DataQualityResult(\n",
    "            check_name=f\"not_empty_{name}\",\n",
    "            passed=not is_empty,\n",
    "            details={\"is_empty\": is_empty},\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        self.logger.info(str(result))\n",
    "        return result\n",
    "\n",
    "    def check_required_columns(\n",
    "        self, df: DataFrame, required_cols: List[str], name: str\n",
    "    ) -> DataQualityResult:\n",
    "        \"\"\"Check that required columns exist.\"\"\"\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        result = DataQualityResult(\n",
    "            check_name=f\"required_columns_{name}\",\n",
    "            passed=len(missing) == 0,\n",
    "            details={\"missing_columns\": missing, \"total_columns\": len(df.columns)},\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        self.logger.info(str(result))\n",
    "        return result\n",
    "\n",
    "    def check_null_percentage(\n",
    "        self, df: DataFrame, column: str, max_null_pct: float = 0.5, name: str = \"\"\n",
    "    ) -> DataQualityResult:\n",
    "        \"\"\"Check that null percentage is within threshold (using sample for performance).\"\"\"\n",
    "        # Sample for performance on large datasets\n",
    "        sample_df = df.sample(fraction=0.01, seed=42) if df.count() > 100000 else df\n",
    "\n",
    "        total = sample_df.count()\n",
    "        if total == 0:\n",
    "            null_pct = 0.0\n",
    "        else:\n",
    "            nulls = sample_df.filter(col(column).isNull() | (col(column) == \"\")).count()\n",
    "            null_pct = nulls / total\n",
    "\n",
    "        result = DataQualityResult(\n",
    "            check_name=f\"null_check_{column}_{name}\",\n",
    "            passed=null_pct <= max_null_pct,\n",
    "            details={\n",
    "                \"null_percentage\": round(null_pct * 100, 2),\n",
    "                \"threshold\": max_null_pct * 100,\n",
    "            },\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        self.logger.info(str(result))\n",
    "        return result\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of all quality checks.\"\"\"\n",
    "        passed = sum(1 for r in self.results if r.passed)\n",
    "        failed = len(self.results) - passed\n",
    "        return {\n",
    "            \"total_checks\": len(self.results),\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"pass_rate\": round(passed / len(self.results) * 100, 2)\n",
    "            if self.results\n",
    "            else 0,\n",
    "        }\n",
    "\n",
    "    def all_passed(self) -> bool:\n",
    "        \"\"\"Check if all quality checks passed.\"\"\"\n",
    "        return all(r.passed for r in self.results)\n",
    "\n",
    "\n",
    "# Initialize data quality checker\n",
    "dq_checker = DataQualityChecker(logger)\n",
    "logger.info(\"Data Quality Framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4cde1",
   "metadata": {},
   "source": [
    "## 4. Data Ingestion Pipeline\n",
    "\n",
    "Core ETL logic with error handling, metrics collection, and processing orchestration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6624d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:26 | INFO     | Pipeline initialized and ready to run (batch mode enabled)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bronze Layer Ingestion Pipeline - Core ETL Logic\n",
    "\n",
    "Handles THREE schema versions with BATCH PROCESSING to avoid OOM:\n",
    "- V1 (2009): vendor_name, Trip_Pickup_DateTime, Start_Lon\n",
    "- V2 (2010): vendor_id, pickup_datetime, pickup_longitude\n",
    "- V3 (2011+): VendorID, tpep_pickup_datetime, PULocationID\n",
    "\n",
    "Strategy: Write each schema version directly to Delta (append mode),\n",
    "rather than unioning everything in memory.\n",
    "\"\"\"\n",
    "from nyc_taxi_eta.configs.settings import YELLOW_TAXI_DIR, ROOT_DIR\n",
    "from pyspark.sql.functions import year, to_timestamp\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineMetrics:\n",
    "    \"\"\"Metrics collected during pipeline execution.\"\"\"\n",
    "\n",
    "    start_time: datetime = field(default_factory=datetime.now)\n",
    "    end_time: Optional[datetime] = None\n",
    "    v1_files: int = 0\n",
    "    v2_files: int = 0\n",
    "    v3_files: int = 0\n",
    "    total_records_processed: int = 0\n",
    "    processing_duration_seconds: float = 0.0\n",
    "    status: str = \"running\"\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "    def complete(self, status: str = \"success\", error: Optional[str] = None):\n",
    "        \"\"\"Mark pipeline as complete.\"\"\"\n",
    "        self.end_time = datetime.now()\n",
    "        self.status = status\n",
    "        self.error_message = error\n",
    "        self.processing_duration_seconds = (\n",
    "            self.end_time - self.start_time\n",
    "        ).total_seconds()\n",
    "\n",
    "\n",
    "class BronzeIngestionPipeline:\n",
    "    \"\"\"\n",
    "    Enterprise-grade Bronze layer ingestion pipeline with BATCH PROCESSING.\n",
    "\n",
    "    Handles:\n",
    "    - THREE schema versions across different data vintages\n",
    "    - Batch processing to avoid memory issues\n",
    "    - Error handling and retry logic\n",
    "    - Data quality validation\n",
    "    - Metrics collection and logging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: pyspark.sql.SparkSession,\n",
    "        config: PipelineConfig,\n",
    "        schema_registry: SchemaRegistry,\n",
    "        dq_checker: DataQualityChecker,\n",
    "        logger: logging.Logger,\n",
    "    ):\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        self.schema_registry = schema_registry\n",
    "        self.dq_checker = dq_checker\n",
    "        self.logger = logger\n",
    "        self.metrics = PipelineMetrics()\n",
    "\n",
    "        # Set paths from settings\n",
    "        self.config.source_dir = YELLOW_TAXI_DIR\n",
    "        self.config.target_dir = ROOT_DIR / \"data\" / \"bronze\" / \"yellow_taxi\"\n",
    "\n",
    "    def discover_files(self) -> Tuple[List[str], List[str], List[str]]:\n",
    "        \"\"\"Discover and classify source files by schema version.\"\"\"\n",
    "        self.logger.info(f\"Discovering files in: {self.config.source_dir}\")\n",
    "\n",
    "        all_files = sorted(\n",
    "            [\n",
    "                str(self.config.source_dir / f)\n",
    "                for f in os.listdir(self.config.source_dir)\n",
    "                if f.endswith(\".parquet\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Classify by year\n",
    "        v1_files = [f for f in all_files if any(y in f for y in self.config.v1_years)]\n",
    "        v2_files = [f for f in all_files if any(y in f for y in self.config.v2_years)]\n",
    "        v3_files = [f for f in all_files if f not in v1_files and f not in v2_files]\n",
    "\n",
    "        self.metrics.v1_files = len(v1_files)\n",
    "        self.metrics.v2_files = len(v2_files)\n",
    "        self.metrics.v3_files = len(v3_files)\n",
    "\n",
    "        self.logger.info(f\"Found {len(v1_files)} V1 files (2009)\")\n",
    "        self.logger.info(f\"Found {len(v2_files)} V2 files (2010)\")\n",
    "        self.logger.info(f\"Found {len(v3_files)} V3 files (2011+)\")\n",
    "\n",
    "        return v1_files, v2_files, v3_files\n",
    "\n",
    "    def _process_and_write_batch(\n",
    "        self, \n",
    "        files: List[str], \n",
    "        transform_fn, \n",
    "        batch_name: str,\n",
    "        write_mode: str\n",
    "    ) -> None:\n",
    "        \"\"\"Process a batch of files and write directly to Delta.\"\"\"\n",
    "        if not files:\n",
    "            return\n",
    "            \n",
    "        self.logger.info(f\"Processing {len(files)} files for {batch_name}...\")\n",
    "        \n",
    "        # Read files WITHOUT merging schema - each file read separately then unioned\n",
    "        # This avoids Spark trying to reconcile types across files\n",
    "        dfs = []\n",
    "        for f in files:\n",
    "            df = self.spark.read.parquet(f)\n",
    "            df_transformed = transform_fn(df)\n",
    "            dfs.append(df_transformed)\n",
    "        \n",
    "        # Union all transformed DataFrames (all have same STRING schema now)\n",
    "        df_bronze = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            df_bronze = df_bronze.unionByName(df)\n",
    "        \n",
    "        # Add partition column\n",
    "        df_bronze = df_bronze.withColumn(\n",
    "            \"year\", year(to_timestamp(col(\"pickup_datetime\")))\n",
    "        )\n",
    "        \n",
    "        # NOTE: No data quality filtering at Bronze layer - preserve raw data as-is\n",
    "        # Invalid years (e.g., 2001, 2084) will be filtered at Silver layer\n",
    "        \n",
    "        # Write to Delta with mergeSchema to handle any variations\n",
    "        df_bronze.write.format(\"delta\") \\\n",
    "            .mode(write_mode) \\\n",
    "            .partitionBy(*self.config.partition_by) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .save(str(self.config.target_dir))\n",
    "        \n",
    "        self.logger.info(f\"{batch_name} written to Delta\")\n",
    "        \n",
    "        # Clear cache and trigger GC\n",
    "        del dfs, df_bronze\n",
    "        self.spark.catalog.clearCache()\n",
    "        gc.collect()\n",
    "\n",
    "    def process_v1_format(self, files: List[str], write_mode: str = \"overwrite\") -> None:\n",
    "        \"\"\"Process V1 (2009) format files in batches.\"\"\"\n",
    "        if not files:\n",
    "            return\n",
    "\n",
    "        self.logger.info(\"Processing V1 (2009) format files...\")\n",
    "        batch_size = self.config.batch_size\n",
    "        \n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch = files[i:i + batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "            total_batches = (len(files) + batch_size - 1) // batch_size\n",
    "            \n",
    "            self.logger.info(f\"V1 batch {batch_num}/{total_batches}: {len(batch)} files\")\n",
    "            \n",
    "            # First batch uses specified mode, subsequent use append\n",
    "            mode = write_mode if i == 0 else \"append\"\n",
    "            self._process_and_write_batch(\n",
    "                batch, \n",
    "                self.schema_registry.transform_v1_2009,\n",
    "                f\"V1_batch_{batch_num}\",\n",
    "                mode\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"V1 format processing complete\")\n",
    "\n",
    "    def process_v2_format(self, files: List[str], write_mode: str = \"append\") -> None:\n",
    "        \"\"\"Process V2 (2010) format files in batches.\"\"\"\n",
    "        if not files:\n",
    "            return\n",
    "\n",
    "        self.logger.info(\"Processing V2 (2010) format files...\")\n",
    "        batch_size = self.config.batch_size\n",
    "        \n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch = files[i:i + batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "            total_batches = (len(files) + batch_size - 1) // batch_size\n",
    "            \n",
    "            self.logger.info(f\"V2 batch {batch_num}/{total_batches}: {len(batch)} files\")\n",
    "            \n",
    "            mode = write_mode if i == 0 else \"append\"\n",
    "            self._process_and_write_batch(\n",
    "                batch,\n",
    "                self.schema_registry.transform_v2_2010,\n",
    "                f\"V2_batch_{batch_num}\",\n",
    "                mode\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"V2 format processing complete\")\n",
    "\n",
    "    def process_v3_format(self, files: List[str], write_mode: str = \"append\") -> None:\n",
    "        \"\"\"Process V3 (2011+) format files in batches.\"\"\"\n",
    "        if not files:\n",
    "            return\n",
    "\n",
    "        self.logger.info(\"Processing V3 (2011+) format files...\")\n",
    "        batch_size = self.config.batch_size\n",
    "        \n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch = files[i:i + batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "            total_batches = (len(files) + batch_size - 1) // batch_size\n",
    "            \n",
    "            self.logger.info(f\"V3 batch {batch_num}/{total_batches}: {len(batch)} files\")\n",
    "            \n",
    "            mode = write_mode if i == 0 else \"append\"\n",
    "            self._process_and_write_batch(\n",
    "                batch,\n",
    "                self.schema_registry.transform_v3_new,\n",
    "                f\"V3_batch_{batch_num}\",\n",
    "                mode\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"V3 format processing complete\")\n",
    "\n",
    "    def cleanup_target(self) -> None:\n",
    "        \"\"\"Remove existing Delta table to start fresh.\"\"\"\n",
    "        target = self.config.target_dir\n",
    "        if target.exists():\n",
    "            self.logger.info(f\"Removing existing Delta table: {target}\")\n",
    "            shutil.rmtree(target)\n",
    "\n",
    "    def run(self) -> PipelineMetrics:\n",
    "        \"\"\"Execute the full ingestion pipeline with batch processing.\"\"\"\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        self.logger.info(\"STARTING BRONZE INGESTION PIPELINE (BATCH MODE)\")\n",
    "        self.logger.info(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            # Step 0: Clean up existing table for fresh start\n",
    "            self.cleanup_target()\n",
    "\n",
    "            # Step 1: Discover files\n",
    "            v1_files, v2_files, v3_files = self.discover_files()\n",
    "\n",
    "            # Step 2: Process each schema version and write directly to Delta\n",
    "            # V1 starts fresh (overwrite), subsequent append\n",
    "            self.process_v1_format(v1_files, write_mode=\"overwrite\")\n",
    "            self.process_v2_format(v2_files, write_mode=\"append\")\n",
    "            self.process_v3_format(v3_files, write_mode=\"append\")\n",
    "\n",
    "            # Step 3: Validate output\n",
    "            self.logger.info(\"Validating Delta table...\")\n",
    "            df_check = self.spark.read.format(\"delta\").load(str(self.config.target_dir))\n",
    "            self.dq_checker.check_not_empty(df_check, \"bronze_output\")\n",
    "            self.dq_checker.check_required_columns(\n",
    "                df_check, SchemaRegistry.BRONZE_COLUMNS + [\"year\"], \"bronze_output\"\n",
    "            )\n",
    "\n",
    "            # Complete metrics\n",
    "            self.metrics.complete(status=\"success\")\n",
    "            self.logger.info(\"Pipeline completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            self.metrics.complete(status=\"failed\", error=str(e))\n",
    "            raise\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = BronzeIngestionPipeline(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    schema_registry=schema_registry,\n",
    "    dq_checker=dq_checker,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "logger.info(\"Pipeline initialized and ready to run (batch mode enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12229f4e",
   "metadata": {},
   "source": [
    "## 5. Pipeline Execution\n",
    "\n",
    "Execute the pipeline with optional preview mode for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8611b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:26 | INFO     | Discovering files in: /home/administrator/Desktop/datascience/github/nyc-taxi-eta/data/landing/trips_data/yellow_taxi\n",
      "2025-11-29 14:40:26 | INFO     | Found 12 V1 files (2009)\n",
      "2025-11-29 14:40:26 | INFO     | Found 12 V2 files (2010)\n",
      "2025-11-29 14:40:26 | INFO     | Found 178 V3 files (2011+)\n",
      "2025-11-29 14:40:26 | INFO     | Preview: V1 (2009) format sample\n",
      "2025-11-29 14:40:26 | INFO     | Found 12 V1 files (2009)\n",
      "2025-11-29 14:40:26 | INFO     | Found 12 V2 files (2010)\n",
      "2025-11-29 14:40:26 | INFO     | Found 178 V3 files (2011+)\n",
      "2025-11-29 14:40:26 | INFO     | Preview: V1 (2009) format sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|pulocationid|dolocationid|improvement_surcharge|congestion_surcharge|airport_fee|cbd_congestion_fee|                   source_file|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|      VTS|2009-01-04 02:52:00|2009-01-04 03:02:00|              1|         2.63|      -73.991957|      40.721567|       -73.993803|       40.695922|     NULL|              NULL|        CASH|        8.9|  0.5|   NULL|       0.0|         0.0|         9.4|        NULL|        NULL|                 NULL|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|      VTS|2009-01-04 03:31:00|2009-01-04 03:38:00|              3|         4.55|      -73.982102|       40.73629|        -73.95585|        40.76803|     NULL|              NULL|      Credit|       12.1|  0.5|   NULL|       2.0|         0.0|        14.6|        NULL|        NULL|                 NULL|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|      VTS|2009-01-03 15:43:00|2009-01-03 15:57:00|              5|        10.35|      -74.002587|      40.739748|       -73.869983|       40.770225|     NULL|              NULL|      Credit|       23.7|  0.0|   NULL|      4.74|         0.0|       28.44|        NULL|        NULL|                 NULL|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "\n",
      "2025-11-29 14:40:30 | INFO     | Preview: V2 (2010) format sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+------------------+---------------+------------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|pickup_latitude| dropoff_longitude|dropoff_latitude|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|pulocationid|dolocationid|improvement_surcharge|congestion_surcharge|airport_fee|cbd_congestion_fee|                   source_file|\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------------+---------------+------------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|      VTS|2010-01-26 07:41:00|2010-01-26 07:45:00|              1|         0.75|        -73.956778|       40.76775|        -73.965957|       40.765232|        1|              NULL|         CAS|        4.5|  0.0|    0.5|       0.0|         0.0|         5.0|        NULL|        NULL|                 NULL|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|      DDS|2010-01-30 23:31:00|2010-01-30 23:46:12|              1|          5.9|-73.99611799999998|      40.763932|-73.98151199999998|       40.741193|        1|              NULL|         CAS|       15.3|  0.5|    0.5|       0.0|         0.0|        16.3|        NULL|        NULL|                 NULL|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|      DDS|2010-01-18 20:22:20|2010-01-18 20:38:12|              1|          4.0|        -73.979673|       40.78379|-73.91785199999998|        40.87856|        1|              NULL|         CAS|       11.7|  0.5|    0.5|       0.0|         0.0|        12.7|        NULL|        NULL|                 NULL|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "+---------+-------------------+-------------------+---------------+-------------+------------------+---------------+------------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "\n",
      "2025-11-29 14:40:31 | INFO     | Preview: V3 (2011+) format sample\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|pulocationid|dolocationid|improvement_surcharge|congestion_surcharge|airport_fee|cbd_congestion_fee|                   source_file|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|        2|2011-01-01 00:10:00|2011-01-01 00:12:00|              4|          0.0|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|        2.9|  0.5|    0.5|      0.28|         0.0|        4.18|         145|         145|                  0.0|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|        2|2011-01-01 00:04:00|2011-01-01 00:13:00|              4|          0.0|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|        5.7|  0.5|    0.5|      0.24|         0.0|        6.94|         264|         264|                  0.0|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|        2|2011-01-01 00:14:00|2011-01-01 00:16:00|              4|          0.0|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|        2.9|  0.5|    0.5|      1.11|         0.0|        5.01|         264|         264|                  0.0|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "\n",
      "2025-11-29 14:40:32 | INFO     | Preview complete - ready for full pipeline execution\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|pulocationid|dolocationid|improvement_surcharge|congestion_surcharge|airport_fee|cbd_congestion_fee|                   source_file|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "|        2|2011-01-01 00:10:00|2011-01-01 00:12:00|              4|          0.0|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|        2.9|  0.5|    0.5|      0.28|         0.0|        4.18|         145|         145|                  0.0|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|        2|2011-01-01 00:04:00|2011-01-01 00:13:00|              4|          0.0|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|        5.7|  0.5|    0.5|      0.24|         0.0|        6.94|         264|         264|                  0.0|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "|        2|2011-01-01 00:14:00|2011-01-01 00:16:00|              4|          0.0|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|        2.9|  0.5|    0.5|      1.11|         0.0|        5.01|         264|         264|                  0.0|                NULL|       NULL|              NULL|file:///home/administrator/...|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+------------------------------+\n",
      "\n",
      "2025-11-29 14:40:32 | INFO     | Preview complete - ready for full pipeline execution\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preview Mode - Test the pipeline without writing data.\n",
    "Run this cell to validate transformations before full execution.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Preview: Discover files\n",
    "v1_files, v2_files, v3_files = pipeline.discover_files()\n",
    "\n",
    "# Preview V1 (2009) format transformation - single file only\n",
    "if v1_files:\n",
    "    logger.info(\"Preview: V1 (2009) format sample\")\n",
    "    df_v1 = spark.read.parquet(v1_files[0])\n",
    "    df_v1_bronze = schema_registry.transform_v1_2009(df_v1)\n",
    "    df_v1_bronze.limit(3).show(truncate=30)\n",
    "    del df_v1, df_v1_bronze\n",
    "\n",
    "# Preview V2 (2010) format transformation - single file only\n",
    "if v2_files:\n",
    "    logger.info(\"Preview: V2 (2010) format sample\")\n",
    "    df_v2 = spark.read.parquet(v2_files[0])\n",
    "    df_v2_bronze = schema_registry.transform_v2_2010(df_v2)\n",
    "    df_v2_bronze.limit(3).show(truncate=30)\n",
    "    del df_v2, df_v2_bronze\n",
    "\n",
    "# Preview V3 (2011+) format transformation - single file only\n",
    "if v3_files:\n",
    "    logger.info(\"Preview: V3 (2011+) format sample\")\n",
    "    df_v3 = spark.read.parquet(v3_files[0])\n",
    "    df_v3_bronze = schema_registry.transform_v3_new(df_v3)\n",
    "    df_v3_bronze.limit(3).show(truncate=30)\n",
    "    del df_v3, df_v3_bronze\n",
    "\n",
    "# Clear any cached data\n",
    "spark.catalog.clearCache()\n",
    "logger.info(\"Preview complete - ready for full pipeline execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d25de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:40:32 | INFO     | ============================================================\n",
      "2025-11-29 14:40:32 | INFO     | STARTING BRONZE INGESTION PIPELINE (BATCH MODE)\n",
      "2025-11-29 14:40:32 | INFO     | ============================================================\n",
      "2025-11-29 14:40:32 | INFO     | Removing existing Delta table: /home/administrator/Desktop/datascience/github/nyc-taxi-eta/data/bronze/yellow_taxi\n",
      "2025-11-29 14:40:32 | INFO     | STARTING BRONZE INGESTION PIPELINE (BATCH MODE)\n",
      "2025-11-29 14:40:32 | INFO     | ============================================================\n",
      "2025-11-29 14:40:32 | INFO     | Removing existing Delta table: /home/administrator/Desktop/datascience/github/nyc-taxi-eta/data/bronze/yellow_taxi\n",
      "2025-11-29 14:40:34 | INFO     | Discovering files in: /home/administrator/Desktop/datascience/github/nyc-taxi-eta/data/landing/trips_data/yellow_taxi\n",
      "2025-11-29 14:40:34 | INFO     | Found 12 V1 files (2009)\n",
      "2025-11-29 14:40:34 | INFO     | Found 12 V2 files (2010)\n",
      "2025-11-29 14:40:34 | INFO     | Found 178 V3 files (2011+)\n",
      "2025-11-29 14:40:34 | INFO     | Processing V1 (2009) format files...\n",
      "2025-11-29 14:40:34 | INFO     | V1 batch 1/1: 12 files\n",
      "2025-11-29 14:40:34 | INFO     | Processing 12 files for V1_batch_1...\n",
      "2025-11-29 14:40:34 | INFO     | Discovering files in: /home/administrator/Desktop/datascience/github/nyc-taxi-eta/data/landing/trips_data/yellow_taxi\n",
      "2025-11-29 14:40:34 | INFO     | Found 12 V1 files (2009)\n",
      "2025-11-29 14:40:34 | INFO     | Found 12 V2 files (2010)\n",
      "2025-11-29 14:40:34 | INFO     | Found 178 V3 files (2011+)\n",
      "2025-11-29 14:40:34 | INFO     | Processing V1 (2009) format files...\n",
      "2025-11-29 14:40:34 | INFO     | V1 batch 1/1: 12 files\n",
      "2025-11-29 14:40:34 | INFO     | Processing 12 files for V1_batch_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 14:40:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:44:57 | INFO     | V1_batch_1 written to Delta\n",
      "2025-11-29 14:44:57 | INFO     | V1 format processing complete\n",
      "2025-11-29 14:44:57 | INFO     | Processing V2 (2010) format files...\n",
      "2025-11-29 14:44:57 | INFO     | V2 batch 1/1: 12 files\n",
      "2025-11-29 14:44:57 | INFO     | Processing 12 files for V2_batch_1...\n",
      "2025-11-29 14:44:57 | INFO     | V1 format processing complete\n",
      "2025-11-29 14:44:57 | INFO     | Processing V2 (2010) format files...\n",
      "2025-11-29 14:44:57 | INFO     | V2 batch 1/1: 12 files\n",
      "2025-11-29 14:44:57 | INFO     | Processing 12 files for V2_batch_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:49:23 | INFO     | V2_batch_1 written to Delta\n",
      "2025-11-29 14:49:23 | INFO     | V2 format processing complete\n",
      "2025-11-29 14:49:23 | INFO     | Processing V3 (2011+) format files...\n",
      "2025-11-29 14:49:23 | INFO     | V3 batch 1/9: 20 files\n",
      "2025-11-29 14:49:23 | INFO     | Processing 20 files for V3_batch_1...\n",
      "2025-11-29 14:49:23 | INFO     | V2 format processing complete\n",
      "2025-11-29 14:49:23 | INFO     | Processing V3 (2011+) format files...\n",
      "2025-11-29 14:49:23 | INFO     | V3 batch 1/9: 20 files\n",
      "2025-11-29 14:49:23 | INFO     | Processing 20 files for V3_batch_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:54:43 | INFO     | V3_batch_1 written to Delta\n",
      "2025-11-29 14:54:44 | INFO     | V3 batch 2/9: 20 files\n",
      "2025-11-29 14:54:44 | INFO     | Processing 20 files for V3_batch_2...\n",
      "2025-11-29 14:54:44 | INFO     | V3 batch 2/9: 20 files\n",
      "2025-11-29 14:54:44 | INFO     | Processing 20 files for V3_batch_2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:00:03 | INFO     | V3_batch_2 written to Delta\n",
      "2025-11-29 15:00:03 | INFO     | V3 batch 3/9: 20 files\n",
      "2025-11-29 15:00:03 | INFO     | Processing 20 files for V3_batch_3...\n",
      "2025-11-29 15:00:03 | INFO     | V3 batch 3/9: 20 files\n",
      "2025-11-29 15:00:03 | INFO     | Processing 20 files for V3_batch_3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:04:31 | INFO     | V3_batch_3 written to Delta\n",
      "2025-11-29 15:04:32 | INFO     | V3 batch 4/9: 20 files\n",
      "2025-11-29 15:04:32 | INFO     | Processing 20 files for V3_batch_4...\n",
      "2025-11-29 15:04:32 | INFO     | V3 batch 4/9: 20 files\n",
      "2025-11-29 15:04:32 | INFO     | Processing 20 files for V3_batch_4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:08:16 | INFO     | V3_batch_4 written to Delta\n",
      "2025-11-29 15:08:16 | INFO     | V3 batch 5/9: 20 files\n",
      "2025-11-29 15:08:16 | INFO     | Processing 20 files for V3_batch_5...\n",
      "2025-11-29 15:08:16 | INFO     | V3 batch 5/9: 20 files\n",
      "2025-11-29 15:08:16 | INFO     | Processing 20 files for V3_batch_5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:11:26 | INFO     | V3_batch_5 written to Delta\n",
      "2025-11-29 15:11:26 | INFO     | V3 batch 6/9: 20 files\n",
      "2025-11-29 15:11:26 | INFO     | Processing 20 files for V3_batch_6...\n",
      "2025-11-29 15:11:26 | INFO     | V3 batch 6/9: 20 files\n",
      "2025-11-29 15:11:26 | INFO     | Processing 20 files for V3_batch_6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:13:33 | INFO     | V3_batch_6 written to Delta\n",
      "2025-11-29 15:13:34 | INFO     | V3 batch 7/9: 20 files\n",
      "2025-11-29 15:13:34 | INFO     | Processing 20 files for V3_batch_7...\n",
      "2025-11-29 15:13:34 | INFO     | V3 batch 7/9: 20 files\n",
      "2025-11-29 15:13:34 | INFO     | Processing 20 files for V3_batch_7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:15:00 | INFO     | V3_batch_7 written to Delta\n",
      "2025-11-29 15:15:00 | INFO     | V3 batch 8/9: 20 files\n",
      "2025-11-29 15:15:00 | INFO     | Processing 20 files for V3_batch_8...\n",
      "2025-11-29 15:15:00 | INFO     | V3 batch 8/9: 20 files\n",
      "2025-11-29 15:15:00 | INFO     | Processing 20 files for V3_batch_8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:16:41 | INFO     | V3_batch_8 written to Delta\n",
      "2025-11-29 15:16:41 | INFO     | V3 batch 9/9: 18 files\n",
      "2025-11-29 15:16:41 | INFO     | Processing 18 files for V3_batch_9...\n",
      "2025-11-29 15:16:41 | INFO     | V3 batch 9/9: 18 files\n",
      "2025-11-29 15:16:41 | INFO     | Processing 18 files for V3_batch_9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:18:20 | INFO     | V3_batch_9 written to Delta\n",
      "2025-11-29 15:18:20 | INFO     | V3 format processing complete\n",
      "2025-11-29 15:18:20 | INFO     | Validating Delta table...\n",
      "2025-11-29 15:18:20 | INFO     | V3 format processing complete\n",
      "2025-11-29 15:18:20 | INFO     | Validating Delta table...\n",
      "2025-11-29 15:18:20 | INFO     | ✅ PASSED | not_empty_bronze_output: {'is_empty': False}\n",
      "2025-11-29 15:18:20 | INFO     | ✅ PASSED | required_columns_bronze_output: {'missing_columns': [], 'total_columns': 26}\n",
      "2025-11-29 15:18:20 | INFO     | Pipeline completed successfully!\n",
      "2025-11-29 15:18:20 | INFO     | ✅ PASSED | not_empty_bronze_output: {'is_empty': False}\n",
      "2025-11-29 15:18:20 | INFO     | ✅ PASSED | required_columns_bronze_output: {'missing_columns': [], 'total_columns': 26}\n",
      "2025-11-29 15:18:20 | INFO     | Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FULL PIPELINE EXECUTION\n",
    "\n",
    "⚠️ WARNING: This will process ALL files and write to Delta table.\n",
    "   Estimated time: 10-30 minutes depending on data volume.\n",
    "\n",
    "Uncomment and run when ready for production execution.\n",
    "\"\"\"\n",
    "# Execute the full pipeline\n",
    "metrics = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030a65e",
   "metadata": {},
   "source": [
    "## 6. Pipeline Results & Monitoring\n",
    "\n",
    "Review execution metrics, data quality results, and validate output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb4942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:18:20 | INFO     | ============================================================\n",
      "2025-11-29 15:18:20 | INFO     | PIPELINE EXECUTION SUMMARY\n",
      "2025-11-29 15:18:20 | INFO     | ============================================================\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════╗\n",
      "║                    PIPELINE METRICS                          ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║ Status:              SUCCESS                                  ║\n",
      "║ Start Time:          2025-11-29 14:40:26.492860               ║\n",
      "║ End Time:            2025-11-29 15:18:20.645689               ║\n",
      "║ Duration:            2274.15 seconds                               ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║                    FILE STATISTICS                           ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║ V1 Files (2009):     12                                       ║\n",
      "║ V2 Files (2010):     12                                       ║\n",
      "║ V3 Files (2011+):    178                                      ║\n",
      "║ Total Files:         202                                      ║\n",
      "╚══════════════════════════════════════════════════════════════╝\n",
      "\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════╗\n",
      "║                  DATA QUALITY SUMMARY                        ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║ Total Checks:        2                                        ║\n",
      "║ Passed:              2                                        ║\n",
      "║ Failed:              0                                        ║\n",
      "║ Pass Rate:           100.0%                                      ║\n",
      "╚══════════════════════════════════════════════════════════════╝\n",
      "\n",
      "2025-11-29 15:18:20 | INFO     | PIPELINE EXECUTION SUMMARY\n",
      "2025-11-29 15:18:20 | INFO     | ============================================================\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════╗\n",
      "║                    PIPELINE METRICS                          ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║ Status:              SUCCESS                                  ║\n",
      "║ Start Time:          2025-11-29 14:40:26.492860               ║\n",
      "║ End Time:            2025-11-29 15:18:20.645689               ║\n",
      "║ Duration:            2274.15 seconds                               ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║                    FILE STATISTICS                           ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║ V1 Files (2009):     12                                       ║\n",
      "║ V2 Files (2010):     12                                       ║\n",
      "║ V3 Files (2011+):    178                                      ║\n",
      "║ Total Files:         202                                      ║\n",
      "╚══════════════════════════════════════════════════════════════╝\n",
      "\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════╗\n",
      "║                  DATA QUALITY SUMMARY                        ║\n",
      "╠══════════════════════════════════════════════════════════════╣\n",
      "║ Total Checks:        2                                        ║\n",
      "║ Passed:              2                                        ║\n",
      "║ Failed:              0                                        ║\n",
      "║ Pass Rate:           100.0%                                      ║\n",
      "╚══════════════════════════════════════════════════════════════╝\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pipeline Execution Summary and Metrics Report\n",
    "\"\"\"\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Pipeline Metrics Summary\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"PIPELINE EXECUTION SUMMARY\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════╗\n",
    "║                    PIPELINE METRICS                          ║\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "║ Status:              {metrics.status.upper():<40} ║\n",
    "║ Start Time:          {str(metrics.start_time):<40} ║\n",
    "║ End Time:            {str(metrics.end_time):<40} ║\n",
    "║ Duration:            {metrics.processing_duration_seconds:.2f} seconds{\" \":<30} ║\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "║                    FILE STATISTICS                           ║\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "║ V1 Files (2009):     {metrics.v1_files:<40} ║\n",
    "║ V2 Files (2010):     {metrics.v2_files:<40} ║\n",
    "║ V3 Files (2011+):    {metrics.v3_files:<40} ║\n",
    "║ Total Files:         {metrics.v1_files + metrics.v2_files + metrics.v3_files:<40} ║\n",
    "╚══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "# Data Quality Summary\n",
    "dq_summary = dq_checker.get_summary()\n",
    "print(f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════╗\n",
    "║                  DATA QUALITY SUMMARY                        ║\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "║ Total Checks:        {dq_summary[\"total_checks\"]:<40} ║\n",
    "║ Passed:              {dq_summary[\"passed\"]:<40} ║\n",
    "║ Failed:              {dq_summary[\"failed\"]:<40} ║\n",
    "║ Pass Rate:           {dq_summary[\"pass_rate\"]:.1f}%{\" \":<37} ║\n",
    "╚══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f33dc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:18:20 | INFO     | Bronze Table Schema:\n",
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- pulocationid: string (nullable = true)\n",
      " |-- dolocationid: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- airport_fee: string (nullable = true)\n",
      " |-- cbd_congestion_fee: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "2025-11-29 15:18:20 | INFO     | Bronze Table Partitions (Year):\n",
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- pulocationid: string (nullable = true)\n",
      " |-- dolocationid: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- airport_fee: string (nullable = true)\n",
      " |-- cbd_congestion_fee: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "2025-11-29 15:18:20 | INFO     | Bronze Table Partitions (Year):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 356:==========================================>         (309 + 66) / 375]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2001|\n",
      "|2002|\n",
      "|2003|\n",
      "|2004|\n",
      "|2007|\n",
      "|2008|\n",
      "|2009|\n",
      "|2010|\n",
      "|2011|\n",
      "|2012|\n",
      "|2013|\n",
      "|2014|\n",
      "|2015|\n",
      "|2016|\n",
      "|2017|\n",
      "|2018|\n",
      "|2019|\n",
      "|2020|\n",
      "|2021|\n",
      "|2022|\n",
      "|2023|\n",
      "|2024|\n",
      "|2025|\n",
      "|2026|\n",
      "|2028|\n",
      "|2029|\n",
      "|2031|\n",
      "|2032|\n",
      "|2033|\n",
      "|2037|\n",
      "|2038|\n",
      "|2041|\n",
      "|2042|\n",
      "|2053|\n",
      "|2058|\n",
      "|2066|\n",
      "|2070|\n",
      "|2084|\n",
      "|2088|\n",
      "|2090|\n",
      "|2098|\n",
      "+----+\n",
      "\n",
      "2025-11-29 15:18:22 | INFO     | Sample Data from Bronze Table:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+-------------------------+----+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|pulocationid|dolocationid|improvement_surcharge|congestion_surcharge|airport_fee|cbd_congestion_fee|              source_file|year|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+-------------------------+----+\n",
      "|        2|2012-10-01 00:28:00|2012-10-01 00:36:00|              1|         1.26|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           2|        6.0|  0.5|    0.5|       0.0|         0.0|         7.0|          79|         114|                  0.0|                NULL|       NULL|              NULL|file:///home/administr...|2012|\n",
      "|        1|2012-10-28 05:19:15|2012-10-28 05:39:13|              1|          6.2|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       21.5|  0.5|    0.5|       0.0|         0.0|        22.5|         151|          79|                  0.0|                NULL|       NULL|              NULL|file:///home/administr...|2012|\n",
      "|        1|2012-10-01 00:14:54|2012-10-01 00:30:30|              1|          2.8|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       12.5|  0.5|    0.5|       0.0|         0.0|        13.5|         158|         261|                  0.0|                NULL|       NULL|              NULL|file:///home/administr...|2012|\n",
      "|        1|2012-10-28 05:47:27|2012-10-28 05:54:39|              1|          1.7|            NULL|           NULL|             NULL|            NULL|        1|                 N|           1|        8.0|  0.5|    0.5|       3.0|         0.0|        12.0|         229|         142|                  0.0|                NULL|       NULL|              NULL|file:///home/administr...|2012|\n",
      "|        1|2012-10-01 00:36:18|2012-10-01 00:55:17|              2|          4.5|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       17.5|  0.5|    0.5|       0.0|         0.0|        18.5|          87|         161|                  0.0|                NULL|       NULL|              NULL|file:///home/administr...|2012|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+-------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validate Output: Read back from Delta table and verify.\n",
    "\"\"\"\n",
    "# Read the Delta table\n",
    "BRONZE_DIR = ROOT_DIR / \"data\" / \"bronze\" / \"yellow_taxi\"\n",
    "\n",
    "df_bronze_output = spark.read.format(\"delta\").load(str(BRONZE_DIR))\n",
    "\n",
    "logger.info(\"Bronze Table Schema:\")\n",
    "df_bronze_output.printSchema()\n",
    "\n",
    "logger.info(\"Bronze Table Partitions (Year):\")\n",
    "df_bronze_output.select(\"year\").distinct().orderBy(\"year\").show(50, truncate=False)\n",
    "\n",
    "logger.info(\"Sample Data from Bronze Table:\")\n",
    "df_bronze_output.limit(5).show(truncate=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01a65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:18:22 | INFO     | ============================================================\n",
      "2025-11-29 15:18:22 | INFO     | PIPELINE COMPLETE\n",
      "2025-11-29 15:18:22 | INFO     | Finished at: 2025-11-29T15:18:22.625834\n",
      "2025-11-29 15:18:22 | INFO     | ============================================================\n",
      "2025-11-29 15:18:22 | INFO     | PIPELINE COMPLETE\n",
      "2025-11-29 15:18:22 | INFO     | Finished at: 2025-11-29T15:18:22.625834\n",
      "2025-11-29 15:18:22 | INFO     | ============================================================\n",
      "2025-11-29 15:18:28 | INFO     | Spark session stopped\n",
      "2025-11-29 15:18:28 | INFO     | Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cleanup: Stop Spark session and release resources.\n",
    "\"\"\"\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"PIPELINE COMPLETE\")\n",
    "logger.info(f\"Finished at: {datetime.now().isoformat()}\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Uncomment to stop Spark session\n",
    "spark.stop()\n",
    "logger.info(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-taxi-eta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

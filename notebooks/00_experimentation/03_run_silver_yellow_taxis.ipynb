{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17ed45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:21:30 | INFO     | ============================================================\n",
      "2025-11-29 15:21:30 | INFO     | Pipeline: nyc_yellow_taxi_bronze_ingestion v1.3.0\n",
      "2025-11-29 15:21:30 | INFO     | Environment: development\n",
      "2025-11-29 15:21:30 | INFO     | Started at: 2025-11-29T15:21:30.015174\n",
      "2025-11-29 15:21:30 | INFO     | Memory: Driver=16g, Executor=80g\n",
      "2025-11-29 15:21:30 | INFO     | CPU: 56/64 cores (8 reserved for OS/GUI)\n",
      "2025-11-29 15:21:30 | INFO     | Batch size: 20 files per batch\n",
      "2025-11-29 15:21:30 | INFO     | Schema versions: V1 (2009), V2 (2010), V3 (2011+)\n",
      "2025-11-29 15:21:30 | INFO     | ============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and environment setup for Bronze layer ingestion.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PIPELINE CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Centralized configuration for the Bronze ingestion pipeline.\"\"\"\n",
    "\n",
    "    # Pipeline metadata\n",
    "    pipeline_name: str = \"nyc_yellow_taxi_bronze_ingestion\"\n",
    "    pipeline_version: str = \"1.3.0\"  # Updated for AMD EPYC optimization\n",
    "    environment: str = field(default_factory=lambda: os.getenv(\"ENV\", \"development\"))\n",
    "\n",
    "    # Spark configuration - AMD EPYC 7742 (64 cores) + 128GB RAM\n",
    "    # Reserve 8 cores for Linux GUI + OS, use 56 for Spark\n",
    "    app_name: str = \"NYCTaxiBronze\"\n",
    "    driver_memory: str = \"16g\"    # Driver just coordinates\n",
    "    executor_memory: str = \"80g\"  # Executor does heavy lifting\n",
    "    executor_cores: int = 56      # 64 - 8 reserved for OS/GUI\n",
    "    parallelism: int = 56         # Match available cores\n",
    "    shuffle_partitions: int = 112 # 2x cores for shuffle operations\n",
    "    \n",
    "    # Batch processing - process files in chunks to avoid OOM\n",
    "    batch_size: int = 20  # Process 20 files at a time\n",
    "\n",
    "    # Data paths (will be set from settings)\n",
    "    source_dir: Optional[Path] = None\n",
    "    target_dir: Optional[Path] = None\n",
    "\n",
    "    # Processing options\n",
    "    write_mode: str = \"overwrite\"  # overwrite, append\n",
    "    partition_by: List[str] = field(default_factory=lambda: [\"year\"])\n",
    "    enable_data_quality_checks: bool = True\n",
    "\n",
    "    # Schema version detection by year\n",
    "    # V1: 2009 (vendor_name, Trip_Pickup_DateTime)\n",
    "    # V2: 2010 (vendor_id, pickup_datetime) \n",
    "    # V3: 2011+ (VendorID, tpep_pickup_datetime)\n",
    "    v1_years: List[str] = field(default_factory=lambda: [\"2009\"])\n",
    "    v2_years: List[str] = field(default_factory=lambda: [\"2010\"])\n",
    "    # All other years are V3 (new format)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        assert self.write_mode in [\"overwrite\", \"append\"], (\n",
    "            f\"Invalid write_mode: {self.write_mode}\"\n",
    "        )\n",
    "        assert self.driver_memory.endswith(\"g\"), (\n",
    "            \"Memory should be specified in gigabytes (e.g., '8g')\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "def setup_logging(config: PipelineConfig) -> logging.Logger:\n",
    "    \"\"\"Configure logging for the pipeline.\"\"\"\n",
    "    logger = logging.getLogger(config.pipeline_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "\n",
    "    # Console handler with formatting\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\n",
    "        fmt=\"%(asctime)s | %(levelname)-8s | %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Initialize configuration and logger\n",
    "config = PipelineConfig()\n",
    "logger = setup_logging(config)\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Pipeline: {config.pipeline_name} v{config.pipeline_version}\")\n",
    "logger.info(f\"Environment: {config.environment}\")\n",
    "logger.info(f\"Started at: {datetime.now().isoformat()}\")\n",
    "logger.info(f\"Memory: Driver={config.driver_memory}, Executor={config.executor_memory}\")\n",
    "logger.info(f\"CPU: {config.executor_cores}/64 cores (8 reserved for OS/GUI)\")\n",
    "logger.info(f\"Batch size: {config.batch_size} files per batch\")\n",
    "logger.info(\"Schema versions: V1 (2009), V2 (2010), V3 (2011+)\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5812dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:21:39 | INFO     | Initializing Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/administrator/Desktop/datascience/github/nyc-taxi-eta/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/administrator/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/administrator/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-61bf43bc-69c7-4a50-b20b-deb7319d3258;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 151ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-61bf43bc-69c7-4a50-b20b-deb7319d3258\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/11/29 15:21:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:21:44 | INFO     | Spark version: 4.0.0\n",
      "2025-11-29 15:21:44 | INFO     | Driver memory: 16g\n",
      "2025-11-29 15:21:44 | INFO     | Executor memory: 80g\n",
      "2025-11-29 15:21:44 | INFO     | Executor cores: 56\n",
      "2025-11-29 15:21:44 | INFO     | Default parallelism: 56\n",
      "2025-11-29 15:21:44 | INFO     | Shuffle partitions: 112\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Session initialization with Delta Lake support.\n",
    "\n",
    "⚠️ IMPORTANT: You MUST restart the kernel for memory/CPU changes to take effect!\n",
    "   Kernel → Restart Kernel, then run all cells from the beginning.\n",
    "\n",
    "Optimized for: AMD EPYC 7742 (64 cores) + 128GB RAM\n",
    "\"\"\"\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "import pyspark.sql\n",
    "\n",
    "\n",
    "def create_spark_session(config: PipelineConfig) -> pyspark.sql.SparkSession:\n",
    "    \"\"\"\n",
    "    Create and configure Spark session with Delta Lake support.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration object\n",
    "\n",
    "    Returns:\n",
    "        Configured SparkSession\n",
    "    \"\"\"\n",
    "    logger.info(\"Initializing Spark session...\")\n",
    "\n",
    "    builder = (\n",
    "        pyspark.sql.SparkSession.builder.appName(config.app_name)\n",
    "        # Delta Lake extensions\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        # Memory configuration\n",
    "        .config(\"spark.driver.memory\", config.driver_memory)\n",
    "        .config(\"spark.executor.memory\", config.executor_memory)\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .config(\"spark.memory.fraction\", \"0.8\")\n",
    "        .config(\"spark.memory.storageFraction\", \"0.2\")\n",
    "        # Off-heap memory for extra headroom\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "        .config(\"spark.memory.offHeap.size\", \"16g\")\n",
    "        # CPU/Core configuration - AMD EPYC 7742 optimization\n",
    "        .config(\"spark.executor.cores\", str(config.executor_cores))\n",
    "        .config(\"spark.default.parallelism\", str(config.parallelism))\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(config.shuffle_partitions))\n",
    "        # Performance tuning\n",
    "        .config(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "        # Delta optimizations\n",
    "        .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "        .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "        # File handling\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "    )\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"Driver memory: {config.driver_memory}\")\n",
    "    logger.info(f\"Executor memory: {config.executor_memory}\")\n",
    "    logger.info(f\"Executor cores: {config.executor_cores}\")\n",
    "    logger.info(f\"Default parallelism: {config.parallelism}\")\n",
    "    logger.info(f\"Shuffle partitions: {config.shuffle_partitions}\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace83247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 15:24:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+--------------------+----+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|pulocationid|dolocationid|improvement_surcharge|congestion_surcharge|airport_fee|cbd_congestion_fee|         source_file|year|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+--------------------+----+\n",
      "|        2|2012-10-01 00:28:00|2012-10-01 00:36:00|              1|         1.26|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           2|        6.0|  0.5|    0.5|       0.0|         0.0|         7.0|          79|         114|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-28 05:19:15|2012-10-28 05:39:13|              1|          6.2|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       21.5|  0.5|    0.5|       0.0|         0.0|        22.5|         151|          79|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:14:54|2012-10-01 00:30:30|              1|          2.8|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       12.5|  0.5|    0.5|       0.0|         0.0|        13.5|         158|         261|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-28 05:47:27|2012-10-28 05:54:39|              1|          1.7|            NULL|           NULL|             NULL|            NULL|        1|                 N|           1|        8.0|  0.5|    0.5|       3.0|         0.0|        12.0|         229|         142|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:36:18|2012-10-01 00:55:17|              2|          4.5|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       17.5|  0.5|    0.5|       0.0|         0.0|        18.5|          87|         161|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-28 05:12:07|2012-10-28 05:23:22|              1|          3.6|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       12.5|  0.5|    0.5|       0.0|         0.0|        13.5|         164|          87|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:26:46|2012-10-01 00:37:10|              1|          1.8|            NULL|           NULL|             NULL|            NULL|        4|                 N|           2|        9.5|  0.5|    0.5|       0.0|         0.0|        10.5|         148|         261|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        2|2012-10-28 05:03:00|2012-10-28 05:31:00|              1|        17.15|            NULL|           NULL|             NULL|            NULL|        2|              NULL|           1|       52.0|  0.0|    0.5|      15.6|         0.0|        68.1|         170|         132|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:41:03|2012-10-01 00:55:39|              1|          5.1|            NULL|           NULL|             NULL|            NULL|        4|                 N|           1|       17.5|  0.5|    0.5|      4.62|         0.0|       23.12|         231|         112|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-28 05:59:47|2012-10-28 06:24:09|              1|          4.6|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       20.0|  0.5|    0.5|       0.0|         0.0|        21.0|          25|         225|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:05:48|2012-10-01 00:19:29|              1|          3.6|            NULL|           NULL|             NULL|            NULL|        1|                 N|           1|       13.0|  0.5|    0.5|       2.8|         0.0|        16.8|          49|         211|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        2|2012-10-28 05:26:00|2012-10-28 05:44:00|              1|         6.21|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           2|       20.5|  0.5|    0.5|       0.0|         0.0|        21.5|          79|         188|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:37:35|2012-10-01 00:41:41|              1|          1.0|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|        5.0|  0.5|    0.5|       0.0|         0.0|         6.0|          45|         261|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        2|2012-10-28 05:09:00|2012-10-28 05:31:00|              1|        13.61|            NULL|           NULL|             NULL|            NULL|        1|              NULL|           1|       37.5|  0.5|    0.5|      11.4|         2.2|        52.1|         249|         200|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:07:43|2012-10-01 00:25:13|              2|          5.4|            NULL|           NULL|             NULL|            NULL|        1|                 N|           1|       18.5|  0.5|    0.5|       3.9|         0.0|        23.4|         148|          61|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-28 05:11:08|2012-10-28 05:29:20|              1|          8.1|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       25.0|  0.5|    0.5|       0.0|         0.0|        26.0|          48|         243|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:45:29|2012-10-01 00:50:49|              2|          1.3|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|        6.5|  0.5|    0.5|       0.0|         0.0|         7.5|         145|         146|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-28 05:06:30|2012-10-28 05:40:29|              1|         18.1|            NULL|           NULL|             NULL|            NULL|        5|                 N|           1|       90.0|  0.0|    0.0|      18.0|         0.0|       108.0|         162|           1|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        1|2012-10-01 00:12:45|2012-10-01 00:22:22|              1|          2.8|            NULL|           NULL|             NULL|            NULL|        1|                 N|           2|       11.0|  0.5|    0.5|       0.0|         0.0|        12.0|         142|         262|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "|        2|2012-10-28 05:00:00|2012-10-28 05:01:00|              1|          0.0|            NULL|           NULL|             NULL|            NULL|        5|              NULL|           1|       12.5|  0.0|    0.0|       0.0|         0.0|        12.5|         102|         102|                  0.0|                NULL|       NULL|              NULL|file:///home/admi...|2012|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+-----------------+----------------+---------+------------------+------------+-----------+-----+-------+----------+------------+------------+------------+------------+---------------------+--------------------+-----------+------------------+--------------------+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/home/administrator/Desktop/datascience/github/nyc-taxi-eta/data/bronze/yellow_taxi\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae314b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1', '3', '5', '4', '6', '7', 'VTS', 'DDS', 'CMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# unique vendor ids to list\n",
    "vendor_ids = df.select(\"vendor_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(vendor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3c60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-taxi-eta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
